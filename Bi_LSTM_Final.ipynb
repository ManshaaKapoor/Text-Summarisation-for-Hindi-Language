{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIIX_LmPHNv1",
        "outputId": "6150d58b-aff9-4c70-b3eb-2b9f2e94ac71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.models import Sequential"
      ],
      "metadata": {
        "id": "lSc5_G-bHQQW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test_df = pd.read_csv('/content/drive/My Drive/HindiNews_test.csv')\n",
        "train_df = pd.read_csv('/content/drive/My Drive/hindi_train.csv')"
      ],
      "metadata": {
        "id": "d0WMCYXbHSrN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "train_df = train_df.head(100)\n",
        "#test_df = train_df.head(10)\n",
        "\n",
        "X_train = train_df['Article']\n",
        "y_train = train_df['Summary']\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_train_pad = pad_sequences(X_train_seq, padding='post')\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=X_train_pad.shape[1]))\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "model.fit(X_train_pad, X_train_pad, epochs=1, batch_size=1)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Save the model\n",
        "model.save('bi_lstm_seq2seq_model.h5')"
      ],
      "metadata": {
        "id": "ICTkuuz3HVg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.head(10000)\n",
        "#test_df = train_df.head(10)\n",
        "\n",
        "X_train = train_df['Article']\n",
        "y_train = train_df['Summary']\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_train_pad = pad_sequences(X_train_seq, padding='post')"
      ],
      "metadata": {
        "id": "SCLb8zrjHY2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model('/content/drive/MyDrive/bi_lstm_seq2seq_model.h5')"
      ],
      "metadata": {
        "id": "0NuO17wDHeo9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_df = train_df.head(1)\n",
        "X_test = test_df['Article']\n",
        "y_test = test_df['Summary']\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_pad = pad_sequences(X_test_seq, padding='post', maxlen=X_train_pad.shape[1])\n",
        "\n",
        "y_pred_prob = model.predict(X_test_pad)\n",
        "y_pred = np.argmax(y_pred_prob, axis=2)\n",
        "print(\"Prediction on test data complete.\")\n",
        "\n",
        "summary = tokenizer.sequences_to_texts(y_pred)\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "fxZJZWu1Hiso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Layer\n",
        "from tensorflow.keras.models import Sequential\n",
        "import pandas as pd\n",
        "\n",
        "class CustomAttention(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(CustomAttention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W_heading = self.add_weight(shape=(input_shape[0][-1], 1), initializer=\"normal\", trainable=True)\n",
        "        self.W_article = self.add_weight(shape=(input_shape[1][-1], 1), initializer=\"normal\", trainable=True)\n",
        "        super(CustomAttention, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        heading_encodings, article_encodings = inputs\n",
        "        heading_attention_weights = tf.nn.softmax(tf.matmul(heading_encodings, self.W_heading), axis=1)\n",
        "        article_attention_weights = tf.nn.softmax(tf.matmul(article_encodings, self.W_article), axis=1)\n",
        "        heading_weighted = tf.matmul(tf.transpose(heading_attention_weights, perm=[0, 2, 1]), heading_encodings)\n",
        "        article_weighted = tf.matmul(tf.transpose(article_attention_weights, perm=[0, 2, 1]), article_encodings)\n",
        "        combined_encodings = tf.concat([heading_weighted, article_weighted], axis=1)\n",
        "        return combined_encodings\n",
        "\n",
        "train_df = pd.read_csv(\"hindi_train.csv\")\n",
        "train_df = train_df.head(100)\n",
        "\n",
        "X_train_heading = train_df['Heading']\n",
        "X_train_article = train_df['Article']\n",
        "y_train = train_df['Summary']\n",
        "\n",
        "tokenizer_heading = Tokenizer()\n",
        "tokenizer_heading.fit_on_texts(X_train_heading)\n",
        "X_train_heading_seq = tokenizer_heading.texts_to_sequences(X_train_heading)\n",
        "X_train_heading_pad = pad_sequences(X_train_heading_seq, padding='post')\n",
        "\n",
        "tokenizer_article = Tokenizer()\n",
        "tokenizer_article.fit_on_texts(X_train_article)\n",
        "X_train_article_seq = tokenizer_article.texts_to_sequences(X_train_article)\n",
        "X_train_article_pad = pad_sequences(X_train_article_seq, padding='post')\n",
        "\n",
        "vocab_size = len(tokenizer_heading.word_index) + 1\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=X_train_heading_pad.shape[1]))\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "model.add(CustomAttention())\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "model.fit([X_train_heading_pad, X_train_article_pad], X_train_article_pad, epochs=1, batch_size=1)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "model.save('bi_lstm_attention_seq2seq_model.h5')"
      ],
      "metadata": {
        "id": "LC9upn9nepeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge bert-score seqeval"
      ],
      "metadata": {
        "id": "CaGZn82ZHwY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "generated_summary = summary\n",
        "target_summary = test_df['Summary'].tolist()\n",
        "\n",
        "rouge1_scores = []\n",
        "rouge2_scores = []\n",
        "rouge4_scores = []\n",
        "bert_scores = []\n",
        "\n",
        "rouge = Rouge()\n",
        "rouge_scores = rouge.get_scores(generated_summary, target_summary)[0]\n",
        "\n",
        "rouge4 = (rouge_scores['rouge-1']['f'] * rouge_scores['rouge-2']['f']) ** (1/2)\n",
        "\n",
        "rouge1_scores.append(rouge_scores['rouge-1']['f'])\n",
        "rouge2_scores.append(rouge_scores['rouge-2']['f'])\n",
        "rouge4_scores.append(rouge4)\n",
        "\n",
        "print(\"ROUGE-L (n=1) F1 Score:\", rouge_scores['rouge-1']['f'])\n",
        "print(\"ROUGE-L (n=2) F1 Score:\", rouge_scores['rouge-2']['f'])\n",
        "print(\"ROUGE-4 F1 Score:\", rouge4)"
      ],
      "metadata": {
        "id": "PK7vpttoHlJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bert_score import score as bert_score\n",
        "\n",
        "bert_p, bert_r, bert_f1 = bert_score([generated_summary[0]], [target_summary[0]], lang='hi', verbose=False)\n",
        "bert_scores.append(bert_f1.mean().item())\n",
        "\n",
        "print(\"BERTScore F1 Score:\", bert_f1.mean().item())"
      ],
      "metadata": {
        "id": "CmUEnayTKjH-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}