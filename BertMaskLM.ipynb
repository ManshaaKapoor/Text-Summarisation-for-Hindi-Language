{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oARQqqV9yKn8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b3a4644-f35a-4a3f-806e-865d323ba33b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers[torch]\n"
      ],
      "metadata": {
        "id": "OIrwB2tRXTf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install accelerate -U\n"
      ],
      "metadata": {
        "id": "9QtrGR_qXWGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, articles, summaries, tokenizer, max_length=512):\n",
        "        self.articles = articles\n",
        "        self.summaries = summaries\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.articles)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        article = self.articles[idx]\n",
        "        summary = self.summaries[idx]\n",
        "\n",
        "        inputs = self.tokenizer(article, summary, truncation=True, padding=\"max_length\", max_length=self.max_length,\n",
        "                                return_tensors=\"pt\")\n",
        "        return inputs\n",
        "\n",
        "df_train = pd.read_csv(\"/content/drive/MyDrive/hindi_train.csv\")\n",
        "train_articles = df_train[\"Article\"].tolist()\n",
        "train_summaries = df_train[\"Summary\"].tolist()\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-multilingual-uncased\")\n",
        "\n",
        "train_dataset = CustomDataset(train_articles, train_summaries, tokenizer)\n",
        "\n",
        "batch_size = 1\n",
        "epochs = 1\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch[\"input_ids\"].squeeze(1)\n",
        "        attention_mask = batch[\"attention_mask\"].squeeze(1)\n",
        "        labels = input_ids.clone()\n",
        "        labels[labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Batch Loss: {loss.item()}\")\n",
        "\n",
        "\n",
        "model.save_pretrained(\"./bert-summarization\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BsYzjkqlLmX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM, AdamW\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_source_length=512, max_target_length=128):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_source_length = max_source_length\n",
        "        self.max_target_length = max_target_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        heading = self.data['Heading'][idx]\n",
        "        article = self.data['Article'][idx]\n",
        "        target_text = self.data['Summary'][idx]\n",
        "\n",
        "        heading_tokens = self.tokenizer.encode_plus(\n",
        "            heading,\n",
        "            max_length=self.max_source_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        article_tokens = self.tokenizer.encode_plus(\n",
        "            article,\n",
        "            max_length=self.max_source_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = torch.cat((heading_tokens['input_ids'], article_tokens['input_ids']), dim=1)\n",
        "        attention_mask = torch.cat((heading_tokens['attention_mask'], article_tokens['attention_mask']), dim=1)\n",
        "\n",
        "        target_tokens = self.tokenizer.encode(\n",
        "            target_text,\n",
        "            max_length=self.max_target_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids.flatten(),\n",
        "            'attention_mask': attention_mask.flatten(),\n",
        "            'decoder_input_ids': target_tokens.flatten()[:-1],\n",
        "            'labels': target_tokens.flatten()[1:]\n",
        "        }\n",
        "\n",
        "train_df = pd.read_csv(\"/content/drive/MyDrive/hindi_train.csv\")\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-multilingual-uncased')\n",
        "\n",
        "train_dataset = CustomDataset(train_df, tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "class CustomAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(CustomAttention, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.heading_attention = nn.Linear(hidden_size, 1)\n",
        "        self.article_attention = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, heading_encodings, article_encodings):\n",
        "        heading_attention_weights = torch.softmax(self.heading_attention(heading_encodings), dim=1)\n",
        "        article_attention_weights = torch.softmax(self.article_attention(article_encodings), dim=1)\n",
        "        heading_weighted = torch.bmm(heading_attention_weights.permute(0, 2, 1), heading_encodings)\n",
        "        article_weighted = torch.bmm(article_attention_weights.permute(0, 2, 1), article_encodings)\n",
        "        combined_encodings = torch.cat((heading_weighted, article_weighted), dim=1)\n",
        "        return combined_encodings\n",
        "\n",
        "attention = CustomAttention(model.config.hidden_size)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "model.train()\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "num_epochs = 1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        decoder_input_ids = batch['decoder_input_ids'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, labels=labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "model.save_pretrained(\"/content/drive/MyDrive/NLP_project\")\n"
      ],
      "metadata": {
        "id": "sw2E1zdfQElP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets rouge-score bert-score"
      ],
      "metadata": {
        "id": "R9FQ5UeB8qNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install Rouge"
      ],
      "metadata": {
        "id": "Z1EMec6p-RCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "from rouge import Rouge\n",
        "from bert_score import score\n",
        "\n",
        "\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/hindi_train.csv\")\n",
        "\n",
        "fine_tuned_model = BertForMaskedLM.from_pretrained(\"/content/drive/MyDrive/NLP_BertMaskLM\")\n",
        "\n",
        "rouge1_scores = []\n",
        "rouge2_scores = []\n",
        "rouge4_scores = []\n",
        "bert_scores = []\n",
        "\n",
        "\n",
        "for index, row in test_df.iterrows():\n",
        "    heading = row['Heading']\n",
        "    article = row['Article']\n",
        "    target_summary = row['Summary']\n",
        "\n",
        "    inputs = tokenizer(heading,article, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = fine_tuned_model.generate(inputs['input_ids'], max_length=150, num_beams=4, length_penalty=2.0, early_stopping=True)\n",
        "\n",
        "\n",
        "    generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
        "\n",
        "    if generated_summary.strip() == \"\" or target_summary.strip() == \"\":\n",
        "        print(f\"One of the summaries is empty for index {index}.\")\n",
        "    else:\n",
        "\n",
        "        rouge = Rouge()\n",
        "        rouge_scores = rouge.get_scores(generated_summary, target_summary)[0]\n",
        "        rouge4 = (rouge_scores['rouge-1']['f'] * rouge_scores['rouge-2']['f']) ** (1/2)\n",
        "        _, _, bert_score = score([generated_summary], [target_summary], lang='hi', verbose=False)\n",
        "        rouge1_scores.append(rouge_scores['rouge-1']['f'])\n",
        "        rouge2_scores.append(rouge_scores['rouge-2']['f'])\n",
        "        rouge4_scores.append(rouge4)\n",
        "        bert_scores.append(bert_score.mean().item())\n",
        "\n",
        "avg_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
        "avg_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
        "avg_rouge4 = sum(rouge4_scores) / len(rouge4_scores)\n",
        "avg_bert = sum(bert_scores) / len(bert_scores)\n",
        "\n",
        "print(\"\\nAverage ROUGE-1 F-score:\", avg_rouge1)\n",
        "print(\"Average ROUGE-2 F-score:\", avg_rouge2)\n",
        "print(\"Average ROUGE-4 F-score:\", avg_rouge4)\n",
        "print(\"Average BERTScore:\", avg_bert)\n"
      ],
      "metadata": {
        "id": "cz1Pusy779Wp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}